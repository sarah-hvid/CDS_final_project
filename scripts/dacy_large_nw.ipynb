{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. script\n",
    "### This script employs DaCy Large's NER on all the letters\n",
    "#### Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DaCy Large can be tricky to install. The first snippet allows you to check if the model is available in your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da_dacy_small_tft-0.0.0\n",
      "da_dacy_medium_tft-0.0.0\n",
      "da_dacy_large_tft-0.0.0\n",
      "da_dacy_small_trf-0.1.0\n",
      "da_dacy_medium_trf-0.1.0\n",
      "da_dacy_large_trf-0.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarah\\anaconda3\\lib\\site-packages\\spacy\\util.py:732: UserWarning: [W095] Model 'en_core_web_sm' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import dacy\n",
    "for model in dacy.models():\n",
    "    print(model)\n",
    "    \n",
    "nlp = dacy.load(\"da_dacy_large_trf-0.1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With that done, we need to gather the TXT files by pointint to the txt folder. Again, replace the necessary parts within the qoutation marks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(os.path.join(os.getcwd(), \"C:\\\\Users\\\\Sarah\\\\Desktop\\\\Cultural_data_science\\\\Exam\\\\ibsen_network\\\\data\\\\txt\", \"BREV*.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, we can run a loop that gathers the text from all the files into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "files = []\n",
    "\n",
    "for file_path in file_list:\n",
    "    with open(file_path, encoding = 'utf8') as f_input:\n",
    "        t = f_input.read()\n",
    "        text.append(t)\n",
    "        file = (''.join([n for n in os.path.basename(file_path)]))\n",
    "        file = re.sub('\\.txt', '', file) \n",
    "        files.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With the text in the correct format, we can now apply DaCy Large's NER feature. We are disabling unnecessary parts of the package, and also specifying that we only want the entities of the type 'person' (PER).\n",
    "##### Note that this chunck will take a while to run depending on your system. Set aside at least an hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = []\n",
    "   \n",
    "for doc in nlp.pipe(text, disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "    in_list = []\n",
    "    for e in doc.ents:\n",
    "        if e.label_ == 'PER':\n",
    "            in_list.append((e.text))\n",
    "    ents.append(in_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now that we have the entities we want, we can create a CSV file and save it. First, you need to change the path to match your environment again. It should point to the CSV folder in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (r\"C:\\\\Users\\\\Sarah\\\\Desktop\\\\Cultural_data_science\\\\Exam\\\\ibsen_network\\\\data\\\\csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Then, we can create the dataframe and save it as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = {'files': files, 'text': text, 'dacy_large': ents}\n",
    "\n",
    "df = pd.DataFrame.from_dict(lists, orient = 'index')\n",
    "df = df.transpose()\n",
    "\n",
    "df.to_csv(os.path.join(r'per_dacy_large.csv'), encoding = 'utf-8', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
